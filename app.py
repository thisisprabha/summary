from flask import Flask, request, jsonify, send_from_directory, send_file
import os
import logging
from werkzeug.utils import secure_filename
import tempfile
import time
import json
import sqlite3
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__, static_folder='static', static_url_path='')

# Initialize OpenAI client lazily
_client = None

def get_openai_client():
    global _client
    if _client is None:
        from openai import OpenAI
        _client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    return _client

# Database setup for saving results
def init_db():
    conn = sqlite3.connect('transcriptions.db')
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS transcriptions
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                  filename TEXT,
                  transcription TEXT,
                  summary TEXT,
                  created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                  file_size INTEGER)''')
    conn.commit()
    return conn

# Ensure directories exist
os.makedirs('Uploads', exist_ok=True)
os.makedirs('Results', exist_ok=True)

@app.route('/')
def index():
    return send_from_directory('static', 'index.html')

@app.route('/upload', methods=['POST'])
def upload_audio():
    try:
        if 'audio' not in request.files:
            return jsonify({'error': 'No audio file provided'}), 400

        file = request.files['audio']
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400

        # Check if OpenAI API key is configured
        if not os.getenv('OPENAI_API_KEY'):
            return jsonify({'error': 'OpenAI API key not configured'}), 500

        # Save uploaded file temporarily
        filename = secure_filename(file.filename)
        timestamp = str(int(time.time()))
        safe_filename = f"{timestamp}_{filename}"
        
        # Save to uploads directory temporarily
        upload_path = os.path.join('Uploads', safe_filename)
        file.save(upload_path)
        file_size = os.path.getsize(upload_path)

        # Create temporary file for OpenAI API
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as temp_file:
            file.seek(0)  # Reset file pointer
            temp_file.write(file.read())
            temp_file_path = temp_file.name

        logger.info(f"Processing audio file: {filename} ({file_size} bytes)")

        # Get OpenAI client and transcribe with simple, clean approach
        client = get_openai_client()
        with open(temp_file_path, 'rb') as audio_file:
            try:
                logger.info("ENGLISH-DIRECT TRANSCRIPTION: Using English model for Tamil-English mixed speech")
                
                # Research-based approach: English language setting works better for Tamil-English code-switching
                transcription_response = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="en",  # Force English - research shows this works better for Tamil-English mixed content
                    prompt="Mixed conversation with Tamil and English words",  # Simple context
                    response_format="text"
                )
                
                transcription = transcription_response.strip()
                logger.info(f"Direct English transcription: {transcription[:100]}...")
                
                logger.info("Direct transcription completed successfully")

                # Simple message formatting (clean bullet points)
                if transcription and len(transcription.strip()) > 0:
                    # Split into sentences and add simple formatting
                    sentences = []
                    current_sentence = ""
                    
                    for char in transcription:
                        current_sentence += char
                        if char in '.!?' or (char == '\n' and current_sentence.strip()):
                            if current_sentence.strip():
                                sentences.append(current_sentence.strip())
                                current_sentence = ""
                    
                    # Add any remaining text
                    if current_sentence.strip():
                        sentences.append(current_sentence.strip())
                    
                    # Simple formatting with clean bullets
                    if len(sentences) > 1:
                        formatted_sentences = []
                        for sentence in sentences:
                            if sentence.strip():
                                formatted_sentences.append(f"‚Ä¢ {sentence.strip()}")
                        transcription = "\n".join(formatted_sentences)
                    
                # Store in database (simple approach)
                conn = init_db()
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO transcriptions (filename, transcription, created_at)
                    VALUES (?, ?, datetime('now'))
                ''', (safe_filename, transcription))
                
                transcription_id = cursor.lastrowid
                conn.commit()
                conn.close()
                
                # Save transcription file for download
                transcription_file = f"Results/transcription_{transcription_id}.txt"
                with open(transcription_file, 'w', encoding='utf-8') as f:
                    f.write(transcription)
                
                response = {
                    'success': True,
                    'transcription': transcription,
                    'transcription_id': transcription_id,
                    'filename': safe_filename,
                    'download_url': f'/download/transcription/{transcription_id}'
                }
                
            except Exception as e:
                logger.error(f"Transcription failed: {str(e)}")
                raise e

        # Clean up temporary files
        try:
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
        except Exception as e:
            logger.warning(f"Failed to clean up temporary files: {str(e)}")

        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"Error processing audio: {str(e)}")
        # Clean up files if they exist
        try:
            if 'temp_file_path' in locals():
                os.unlink(temp_file_path)
            if 'upload_path' in locals() and os.path.exists(upload_path):
                os.unlink(upload_path)
        except:
            pass
        return jsonify({'error': str(e)}), 500

def add_speaker_labels_multilingual(transcription):
    """Enhanced speaker labels for English/Tamil mixed conversations"""
    # Clean up repetitive content first
    transcription = clean_repetitive_content(transcription)
    
    # Split into sentences more intelligently for multilingual content
    import re
    # Enhanced sentence splitting for Tamil and English
    sentences = re.split(r'[.!?‡•§]+', transcription)  # Added Tamil punctuation
    enhanced_lines = []
    current_speaker = 1
    word_count = 0
    
    # Tamil conversation markers that indicate speaker changes
    tamil_question_words = ['‡Æé‡Æ©‡Øç‡Æ©', '‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø', '‡Æé‡Æô‡Øç‡Æï‡Øá', '‡ÆØ‡Ææ‡Æ∞‡Øç', '‡Æé‡Æ™‡Øç‡Æ™‡Øã‡Æ§‡ØÅ']
    english_question_words = ['what', 'how', 'where', 'who', 'when', 'why']
    
    for i, sentence in enumerate(sentences):
        sentence = sentence.strip()
        if not sentence or len(sentence) < 5:  # Skip very short segments
            continue
            
        # Count words in this sentence
        words = sentence.split()
        word_count += len(words)
        
        # Enhanced speaker change detection for multilingual content
        is_question = (
            any(qword in sentence.lower() for qword in tamil_question_words) or
            any(qword in sentence.lower() for qword in english_question_words) or
            sentence.endswith('?')
        )
        
        # Check for conversation starters
        conversation_starters = ['‡ÆÖ‡Æ£‡Øç‡Æ£‡Ææ', '‡Æ®‡Ææ‡Æ©‡Øç', 'hello', 'hi', 'okay', 'yes', 'no', '‡Æ®‡Æ©‡Øç‡Æ±‡Æø', 'thanks']
        has_starter = any(starter in sentence.lower() for starter in conversation_starters)
        
        # Switch speakers based on natural conversation patterns
        if (word_count > 30 or  # After reasonable speech amount
            is_question or      # Questions often indicate speaker change
            has_starter or      # Conversation starters
            (i > 0 and len(words) > 15)):  # Longer responses
            current_speaker = 2 if current_speaker == 1 else 1
            word_count = 0
            
        enhanced_lines.append(f"Person{current_speaker}: {sentence}.")
    
    return '\n\n'.join(enhanced_lines)

def clean_repetitive_content(text):
    """Remove repetitive sentences and clean up transcription"""
    import re
    
    # Split into sentences
    sentences = re.split(r'[.!?]+', text)
    cleaned_sentences = []
    seen_sentences = set()
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        # Normalize sentence for comparison (remove extra spaces, case)
        normalized = ' '.join(sentence.lower().split())
        
        # Skip if we've seen this exact sentence or very similar
        if normalized not in seen_sentences and len(normalized) > 15:
            cleaned_sentences.append(sentence)
            seen_sentences.add(normalized)
        elif len(cleaned_sentences) == 0:  # Keep first sentence even if short
            cleaned_sentences.append(sentence)
    
    return '. '.join(cleaned_sentences)

def detect_language_and_quality(transcription):
    """Analyze transcription language and quality with enhanced Tamil detection"""
    import re
    
    # Enhanced Tamil word detection with more comprehensive Tamil vocabulary
    tamil_indicators = [
        # Common Tamil words
        '‡Æ®‡Ææ‡Æ©‡Øç', '‡Æá‡Æ®‡Øç‡Æ§', '‡ÆÖ‡Æ§‡ØÅ', '‡Æé‡Æ©‡Øç', '‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ', '‡Æö‡ØÜ‡ÆØ‡Øç', '‡Æ™‡Ææ‡Æ∞‡Øç‡Æï‡Øç‡Æï', '‡Æµ‡Æ®‡Øç‡Æ§‡ØÅ', '‡Æ™‡Øã‡Æï', '‡Æö‡Øä‡Æ≤‡Øç‡Æ≤', 
        '‡Æï‡ØÇ‡Æü', '‡ÆÆ‡Æü‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç', '‡Æé‡Æ≤‡Øç‡Æ≤‡Ææ‡ÆÆ‡Øç', '‡ÆÖ‡Æµ‡Æ∞‡Øç', '‡Æ®‡ÆÆ‡Øç‡ÆÆ', '‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç', '‡Æ§‡Ææ‡Æ©‡Øç', '‡Æí‡Æ∞‡ØÅ', '‡ÆÖ‡Æ£‡Øç‡Æ£‡Ææ', 
        '‡Æ®‡Ææ‡ÆØ‡Øç', '‡Æ®‡Æø‡ÆÆ‡Æø‡Æü‡ÆÆ‡Øç', '‡Æö‡Æô‡Øç‡Æï‡ÆÆ‡Øç', '‡Æ™‡Ææ‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç', '‡Æ®‡Æ©‡Øç‡Æ±‡Æø',
        # Additional Tamil words commonly used in speech
        '‡Æé‡Æ©‡Øç‡Æ©', '‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø', '‡Æé‡Æô‡Øç‡Æï‡Øá', '‡ÆØ‡Ææ‡Æ∞‡Øç', '‡Æé‡Æ™‡Øç‡Æ™‡Øã‡Æ§‡ØÅ', '‡Æè‡Æ©‡Øç', '‡Æé‡Æ§‡Øç‡Æ§‡Æ©‡Øà', '‡Æé‡Æ®‡Øç‡Æ§',
        '‡Æµ‡Æ∞', '‡Æ™‡Øã‡Æï', '‡Æá‡Æ≤‡Øç‡Æ≤', '‡ÆÜ‡ÆÆ‡Øç', '‡Æá‡Æ≤‡Øç‡Æ≤‡Øà', '‡Æö‡Æ∞‡Æø', '‡Æì‡Æï‡Øá', '‡Æ™‡Æ£‡Øç‡Æ£', '‡Æ™‡Æ£‡Øç‡Æ±‡Øá‡Æ©‡Øç',
        '‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ', '‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Øá‡Æ©‡Øç', '‡Æµ‡Æ®‡Øç‡Æ§‡ØÅ', '‡Æ™‡Øã‡Æ©', '‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç', '‡ÆÆ‡ØÅ‡Æü‡Æø‡ÆØ‡ØÅ‡ÆÆ‡Øç', '‡Æ§‡ØÜ‡Æ∞‡Æø‡ÆØ‡ØÅ‡ÆÆ‡Øç',
        '‡Æ™‡Øá‡Æö‡ØÅ', '‡Æ™‡Øá‡Æö‡Æ±‡Øá‡Æ©‡Øç', '‡Æï‡Øá‡Æ≥‡Øç', '‡Æö‡Øä‡Æ≤‡Øç', '‡Æ™‡Ææ‡Æ∞‡Øç', '‡Æµ‡Ææ', '‡Æ™‡Øã', '‡Æá‡Æ∞‡ØÅ'
    ]
    
    english_indicators = [
        'the', 'and', 'is', 'was', 'have', 'with', 'that', 'for', 'you', 'are', 'this', 
        'will', 'can', 'about', 'they', 'from', 'there', 'been', 'time', 'would',
        'hello', 'people', 'what', 'doing', 'going', 'record', 'audio', 'show', 
        'speak', 'english', 'tamil', 'now', 'suddenly', 'switched', 'while', 'sure',
        'right', 'eating', 'dosa', 'chutney'
    ]
    
    # Count Tamil and English words with partial matching for Tamil
    words = transcription.lower().split()
    tamil_count = 0
    english_count = 0
    
    for word in words:
        # Tamil matching - check if word contains Tamil characters or words
        if any(tamil_word in word for tamil_word in tamil_indicators):
            tamil_count += 1
        elif word in english_indicators:
            english_count += 1
    
    # Enhanced language classification
    total_detected = tamil_count + english_count
    if total_detected == 0:
        language = "Unknown"
    elif tamil_count > english_count * 1.5:  # Higher threshold for Tamil
        language = "Tamil"
    elif english_count > tamil_count * 1.5:  # Higher threshold for English
        language = "English"
    else:
        language = "Mixed English/Tamil"
    
    # Quality assessment
    total_words = len(words)
    unique_words = len(set(words))
    repetition_ratio = 1 - (unique_words / total_words) if total_words > 0 else 0
    
    if repetition_ratio > 0.7:
        quality = "Poor (High repetition)"
    elif repetition_ratio > 0.5:
        quality = "Fair (Some repetition)"
    else:
        quality = "Good"
    
    return {
        'language': language,
        'quality': quality,
        'total_words': total_words,
        'unique_words': unique_words,
        'repetition_ratio': round(repetition_ratio, 2),
        'tamil_words_detected': tamil_count,
        'english_words_detected': english_count
    }

@app.route('/summarize', methods=['POST'])
def summarize_transcription():
    try:
        data = request.get_json()
        transcription = data.get('transcription', '')
        transcription_id = data.get('transcription_id')
        
        if not transcription:
            return jsonify({'error': 'No transcription provided'}), 400

        if not os.getenv('OPENAI_API_KEY'):
            return jsonify({'error': 'OpenAI API key not configured'}), 500

        logger.info("Generating summary with OpenAI GPT")

        # Enhanced MOM prompt for speaker-aware transcriptions with Tamil/English support
        summary_prompt = f"""
Please analyze the following meeting transcription with speaker identification and create a comprehensive Minutes of Meeting (MOM) summary.

Note: This transcription may contain English and Tamil languages mixed together. Please summarize in English while noting when Tamil was spoken.

## Meeting Summary

### Participants:
- [List speakers identified: Person1, Person2, etc.]

### Key Discussion Points:
- [List main topics discussed with speaker attribution where relevant]

### Important Updates & Announcements:
- [List status updates, announcements, decisions made]

### Action Items / Next Steps:
- [List todo items, assignments, deadlines with person responsible if mentioned]

### Decisions Made:
- [List any concrete decisions or agreements reached]

### Questions & Concerns:
- [List any questions raised or concerns discussed]

### Language Notes:
- [Note if Tamil was spoken and provide brief context]

Speaker Transcription:
{transcription}

Please format the response clearly, focus on actionable items and key decisions, and use speaker attribution where helpful. If Tamil text appears unclear, note it as "[Tamil content - may need verification]".
"""

        client = get_openai_client()
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are an expert meeting secretary who creates clear, actionable meeting summaries in Minutes of Meeting format with awareness of English/Tamil mixed conversations."},
                {"role": "user", "content": summary_prompt}
            ],
            max_tokens=1500,
            temperature=0.3
        )

        summary = response.choices[0].message.content

        # Update database with summary
        if transcription_id:
            conn = init_db()
            c = conn.cursor()
            c.execute("UPDATE transcriptions SET summary = ? WHERE id = ?", (summary, transcription_id))
            conn.commit()
            conn.close()

            # Save summary file
            summary_file = f"Results/summary_{transcription_id}.txt"
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(summary)

        logger.info("Summary generated successfully")

        return jsonify({
            'success': True,
            'summary': summary,
            'download_url': f'/download/summary/{transcription_id}' if transcription_id else None
        })

    except Exception as e:
        logger.error(f"Error generating summary: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/download/transcription/<int:transcription_id>')
def download_transcription(transcription_id):
    """Download transcription file"""
    try:
        transcription_file = f"Results/transcription_{transcription_id}.txt"
        if os.path.exists(transcription_file):
            return send_file(transcription_file, as_attachment=True, download_name=f"transcription_{transcription_id}.txt")
        else:
            return jsonify({'error': 'Transcription file not found'}), 404
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/download/summary/<int:transcription_id>')
def download_summary(transcription_id):
    """Download summary file"""
    try:
        summary_file = f"Results/summary_{transcription_id}.txt"
        if os.path.exists(summary_file):
            return send_file(summary_file, as_attachment=True, download_name=f"summary_{transcription_id}.txt")
        else:
            return jsonify({'error': 'Summary file not found'}), 404
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/history')
def get_history():
    """Get list of all processed transcriptions"""
    try:
        conn = init_db()
        c = conn.cursor()
        c.execute("""SELECT id, filename, created_at, file_size, 
                     CASE WHEN summary IS NOT NULL THEN 1 ELSE 0 END as has_summary
                     FROM transcriptions ORDER BY created_at DESC""")
        
        history = []
        for row in c.fetchall():
            history.append({
                'id': row[0],
                'filename': row[1],
                'created_at': row[2],
                'file_size': row[3],
                'has_summary': bool(row[4]),
                'transcription_url': f'/download/transcription/{row[0]}',
                'summary_url': f'/download/summary/{row[0]}' if row[4] else None
            })
        
        conn.close()
        return jsonify(history)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/health', methods=['GET'])
def health_check():
    try:
        openai_configured = bool(os.getenv('OPENAI_API_KEY'))
        
        # Check database
        conn = init_db()
        c = conn.cursor()
        c.execute("SELECT COUNT(*) FROM transcriptions")
        total_transcriptions = c.fetchone()[0]
        conn.close()
        
        return jsonify({
            'status': 'healthy',
            'openai_configured': openai_configured,
            'service': 'OpenAI Whisper direct English transcription for Tamil-English mixed speech',
            'total_transcriptions': total_transcriptions,
            'features': ['direct_english_transcription', 'tamil_english_mixed_support', 'clean_formatting', 'no_hallucination', 'download_history']
        })
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

def add_speaker_labels_smart(transcription):
    """Simple clean transcription without speaker labels - just add message formatting"""
    # Clean up repetitive content first
    transcription = clean_repetitive_content(transcription)
    
    # Split into sentences and add simple message icons
    import re
    sentences = re.split(r'[.!?‡•§]+', transcription)
    formatted_lines = []
    
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence and len(sentence) > 5:  # Skip very short segments
            # Add simple message icon instead of speaker labels
            formatted_lines.append(f"üí¨ {sentence}.")
    
    return '\n\n'.join(formatted_lines)

def is_single_speaker(transcription):
    """Detect if transcription is likely from a single speaker"""
    import re
    
    # Indicators that suggest single speaker
    single_speaker_phrases = [
        'i am going to', 'what i am going to do', 'i am speaking', 'i switched', 
        'i can speak', 'i am not sure', 'right now i am', 'i speak both',
        '‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡Øã‡Æµ‡Øá‡Æ©‡Øç', '‡Æ®‡Ææ‡Æ©‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ‡Æ™‡Øç ‡Æ™‡Øã‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç', '‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡Øá‡Æö‡ØÅ‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç'
    ]
    
    # Count single speaker indicators
    single_indicators = sum(1 for phrase in single_speaker_phrases 
                          if phrase in transcription.lower())
    
    # Check for conversation patterns (questions and responses)
    questions = len(re.findall(r'[.!?‡•§]', transcription))
    conversation_words = ['hello people', 'what are you doing', 'this is what', 'now i can']
    conversation_count = sum(1 for phrase in conversation_words 
                           if phrase in transcription.lower())
    
    # Single speaker if more self-references than conversation patterns
    return single_indicators > conversation_count or len(transcription.split()) < 50

def add_speaker_labels(transcription):
    """Original speaker labels function - kept for compatibility"""
    return add_speaker_labels_multilingual(transcription)

def is_poor_quality_transcription(text):
    """Simplified quality check - focus on obvious garbled patterns"""
    if not text or len(text.strip()) < 5:
        return True
    
    # Check for excessive repetition (simplified)
    words = text.lower().split()
    if len(words) > 3:
        unique_words = len(set(words))
        repetition_ratio = 1 - (unique_words / len(words))
        if repetition_ratio > 0.8:  # Only flag extreme repetition
            return True
    
    # Check for obvious garbled patterns (simplified list)
    obvious_garbled = [
        'congratulations', 'you will you will', 'franklin',
        '‡Æï‡Æ£‡Øç‡Æü‡ØÅ‡Æ™‡Æø‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡Æ§‡ØÅ', '‡ÆÖ‡Æï‡Æø‡Æ≤‡Øá‡Æ∑', '‡Æè‡ÆÆ‡Øç‡ÆÆ‡ØÅ‡Æ±‡Øà', '‡Æí‡Æ©‡Øç‡Æ±‡Øç‡Æï‡Ææ‡Æ©',
        'kurejji', 'acrylic linked', '‡Æü‡Æµ‡Æ©‡Øç'  # From your recent examples
    ]
    
    text_lower = text.lower()
    garbled_count = sum(1 for pattern in obvious_garbled if pattern in text_lower)
    
    # Only flag as poor if multiple garbled patterns found
    return garbled_count >= 2

def preprocess_audio_for_tamil(file_path):
    """Preprocess audio file for better Tamil transcription (placeholder for future enhancement)"""
    # For now, return the original file path
    # Future: Could add noise reduction, volume normalization, etc.
    return file_path

def detect_likely_tamil_content(file_path):
    """Try to detect if audio file likely contains Tamil content"""
    # Since user is consistently getting garbled Tamil, assume Tamil content
    # This is more effective than trying to analyze audio patterns
    
    # Check filename patterns that might indicate Tamil content
    filename = os.path.basename(file_path).lower()
    tamil_indicators = [
        'tamil', 'ta', 'meeting', 'voice', 'record', 'audio',
        'mya', 'myv', 'ramanujan', 'intellion'  # User's file patterns
    ]
    
    filename_suggests_tamil = any(indicator in filename for indicator in tamil_indicators)
    
    # For this user's case, always return True since they're consistently 
    # speaking Tamil/mixed content and getting garbled results
    logger.info(f"Tamil detection: filename_suggests_tamil={filename_suggests_tamil}, defaulting to True for better Tamil handling")
    
    return True  # Always assume Tamil content for this user's workflow

def contains_tamil_content(text):
    """Check if text contains significant Tamil content"""
    # Look for Tamil script characters
    tamil_chars = 0
    total_chars = 0
    
    for char in text:
        if '\u0B80' <= char <= '\u0BFF':  # Tamil Unicode range
            tamil_chars += 1
        if char.isalpha():
            total_chars += 1
    
    # If more than 30% Tamil characters, consider it Tamil content
    if total_chars > 0:
        tamil_ratio = tamil_chars / total_chars
        return tamil_ratio > 0.3
    
    return False

def translate_to_natural_english(tamil_text, client):
    """Translate Tamil transcription to natural English"""
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system", 
                    "content": "You are a precise translator. Translate ONLY what is actually said in the Tamil text to English. Do NOT add any information, details, or interpretations that are not explicitly present in the original text. Keep English words as they are. Be literal and accurate, avoid creative interpretations or filling in gaps."
                },
                {
                    "role": "user", 
                    "content": f"Translate this Tamil text exactly as spoken, without adding anything: {tamil_text}"
                }
            ],
            max_tokens=800,
            temperature=0.1  # Lower temperature for more consistent, conservative translation
        )
        
        return response.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"Translation failed: {str(e)}")
        return None

if __name__ == '__main__':
    print("üéôÔ∏è Meeting Recorder - DIRECT ENGLISH TRANSCRIPTION")
    print("üìù Features: Audio Upload ‚Üí Direct English ‚Üí No Translation ‚Üí Summary")
    print("üóÉÔ∏è  Accurate: Tamil speech directly transcribed to English (no hallucinations)")
    print("üåç DIRECT-ENGLISH: Speak Tamil, get accurate English transcripts immediately")
    if not os.getenv('OPENAI_API_KEY'):
        print("‚ö†Ô∏è  Warning: OPENAI_API_KEY not set in environment variables")
        print("   Create a .env file with: OPENAI_API_KEY=your_api_key_here")
    print("üåê Server starting at: http://localhost:9000")
    app.run(host='0.0.0.0', port=9000, debug=False) 